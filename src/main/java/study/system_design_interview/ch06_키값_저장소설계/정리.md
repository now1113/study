# 키-값 저장소 설계

## 키-값 저장소

- **정의**: 고유 식별자(키) / 값(문자열/JSON/객체등) 매핑하는 **비관계형 저장소**
- 키와 값 사이의 연결관계를 **"키-값" 쌍(pair)이라고 한다**.
- 키는 성능상의 이유로 짧을수록 좋다.
- **대표 예**: `Dynamo(아마존)`, `Redis`, `Memcached`

## 단일 서버 키-값 저장소
- **구성**: 메모리 **해시 테이블**
- **장점**: 단순, 매우 빠름
- **한계/대응**
  - 메모리 한계: **자주 쓰는 것만 메모리(캐시)**, 나머지는 디스크
  - 데이터 한계: 데이터 압축
> 단일 노드로는 저장 용량, 가용성 한계 -> **분산 설계**필요


## 분산 키-값 저장소
- 별칭: **분산 해시 테이블(DHT)**
- 분산 시스템을 설계할 때는 **CAP 정리**를 이해하고 있어야 한다.

### CAP란
- **일관성(consistency)**: 모든 읽기가 가장 최신 커밋을 본다(선형화/강일관성 관점)
- **가용성(availability)**: 파티션 일부가 죽어도 응답을 반환(오류가 아닌 값)
- **파티션 감내(partition tolerance)**: 네트워크 분할/지연/패킷 손실 상황에서도 시스템 지속
> P가 발생하면 C와 A를 동시에 완벽히 만족 불가 -> C 또는 A를 선택(튜닝)
- **CP 시스템**: 일관성, 파티션 감내 지원 -> 가용성을 희생한다
- **AP 시스템**: 가용성, 파티션 감내 지원 -> 일관성을 희생한다.
- **CA 시스템**: 일관성과 가용성을 지원
  - **통상적으로 네트워크 장애는 피할 수 없는 일**로 여겨지므로, 실세계에 CA 시스템은 존재하지 않는다.

#### 이상적 상태
- 이상적 상태라면 **네트워크가 파티션되는 상황은 절대 일어나지 않을 것**이다.
- 기록된 데이터는 자동적으로 나머지 서버에 복제된다. **데이터 일관성과 가용성 만족**

#### 실세계의 분산 시스템
- 네트워크 분할(Partition)은 피할 수 없음 → CAP에서 C(일관성) vs A(가용성) 중 선택 필요.
- **일관성 선택**
  - 분할 시 쓰기(혹은 일부 읽기) 중단 -> 불일치 예방.
  - 결과: 가용성 저하(오류/대기 반환).
  - 예: 온라인 뱅킹/이체처럼 금전·재고 등 불변조건 강한 도메인.
- **가용성 선택**
  - 분할 시에도 읽기(및 쓰기) 계속 허용.
  - 결과: 낡은/불일치 데이터가 잠시 노출될 수 있음(사후 조치 필요)
  - 예: 소셜 피드/상품 리뷰 등 사용자 경험 우선 도메인.

## 시스템 컴포넌트

### 데이터 파티션
- 대규모 어플리케이션의 경우 전체 데이터를 한 대 서버에 넣는건 불가능하다.
- 단순한 해결책은 데이터를 작은 파티션들로 분할한 다음, 여러대의 서버로 저장하는 것이다.
- **고려사항**:
  - **균등 분산**: 키 편향 없이 데이터를 고르게
  - **리밸런싱 최소화**: 노드 **추가/삭제 시 이동 데이터 최소**
- **해결책**: 안정해시
  - **아이디어**: 키와 노드를 해시 링에 배치, **키는 시계방향 첫 노드**가 담당
  - **장점**
    - **규모 확장 자동화**: 시스템 부하에 따라 서버가 자동으로 추가/삭제 될 수 있음
    - **다양성**: 각 서버의 용량에 맞게 가상도느 수 조정 가능

### 데이터 다중화
- **목적**: 노드/네트워크 장애 상황에서도 가용성·내구성을 확보하기 위해 각 키의 사본을 여러 노드에 저장한다.
- **복제 계수** `N`: 키(또는 파티션)당 저장할 사본 개수. 튜닝 가능한 값.
- **배치 규칙(안정 해시 링)**
  -  키를 해시 링에 매핑하고, 그 지점에서 시계 방향으로 이동하며 만나는 서로 다른 물리 노드에 첫 N개 사본을 저장한다. 
  - 가상 노드(vNode) 중복은 건너뛰고 물리 노드 기준으로 계산한다 

### 일관성
- 정족수(Quorum): `N`(사본 수), `W`(쓰기 정족수), `R`(읽기 정족수)
- **코디네이터(중재자)**: 클라이언트 요청을 받은 노드가 프록시 역할을 하며, 해당 키의 레플리카 노드들에 요청을 전송하고 응답을 모은다.
  - `W = 1`이면 한 대 서버에서만 쓰기 응답을 받으면 성공으로 처리하고, 나머지 서버들의 응답은 기다리지 않아도 된다.
  - `R, W `값에 따라 응답 집계 방식이 달라지며, 코디네이터가 이를 관리한다.
- 
- `R = 1, W = N`: 빠른 읽기 연산에 최적화된 시스템
- `W = 1, R = N`: 빠른 쓰기 연산에 최적화된 시스템
- `W + R > N`: 강한 일관성이 보장됨 (보통 N=3, W=R=2)
- `W + R <= N`: 강한 일관성이 보장되지 않음.

### 일관성 모델
- **일관성 모델**: 분산 환경에서 데이터를 읽을 때 최신성을 어느 수준까지 보장할지 정의하는 규칙.
- **강한 일관성 (Strong Consistency)**
  - 모든 읽기 연산은 항상 가장 최신 쓰기 결과를 반환.
  - 클라이언트는 낡은 데이터를 보지 못함
- **약한 일관성 (Weak Consistency)**
  - 읽기 연산이 항상 최신 데이터를 보장하지 않음.
  - 최근 쓰기가 반영되지 않은 값을 읽을 수 있음.
- **결과적 일관성 (Eventual Consistency)**
  - 특정 키에 새로운 쓰기가 없다면, 시간이 지나면서 모든 사본이 결국 일관된 상태로 수렴.
  - Dynamo, Cassandra 같은 시스템이 채택.

## 일관성 불일치 해소 기법

### 데이터 버저닝
- 각 객체(키)에 대해 **버전 정보**를 유지.
- 새 쓰기가 들어올 때마다 버전이 증가하거나 병렬 버전이 생김.
- 각 버전 데이터는 불변(immutable)이다.
- 충돌이 감지되면 **클라이언트 또는 서버가 버전 간 병합.**

### 벡터 시계
- **정의**: 데이터 객체에 [서버ID, 버전번호] 의 순서쌍을 매단 것.
- **표현**: `D([S1, V1], [S2, V2],...`
  - 예: `D([S1, 3], [S2, 5])` -> 서버 S1에서 3번, 서버 S2에서 5번 갱신됨.
- **동작 원리**
  - **쓰기 발생 시**: 해당 서버의 버전번호(`vi`)를 +1.
  - **읽기 시 비교**:
    - 한 버전의 모든 요소가 다른 버전의 요소보다 크거나 같으면 -> **최신 버전**으로 판정.
    - 서로 비교 불가(일부는 크고, 일부는 작음) -> **동시 갱신 충돌(sibling) 발생.**
- **목적**: 객체의 인과 관계(causality) 를 추적하여,
  - 단순한 타임스탬프 비교보다 정확하게 충돌 여부를 판단할 수 있음.
- **단점**:
  - 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로 클라이언트 구현이 복잡해짐
  - [서버:버전]의 순서쌍 개수가 굉장히 빨리 늘어남


## 장애 처리

### 장애 감지
- **Gossip 프로토콜** 활용 -> 노드 간 주기적으로 상태 교환.
- **장애 판별**: 응답 없음, heartbeat 지연 → 노드를 의심(suspect).

### 일시적 장애 처리
- **Sloppy Quorum**: 원래 담당 노드가 다운되면, 요청을 다른 건강한 노드에 임시 기록.
- **Hinted Handoff**: 임시 기록한 노드는 장애 노드가 복구되면 해당 데이터를 원래 노드로 전달. 
  - 가용성을 유지하면서도 나중에 일관성 회복 가능.

### 영구 장애 처리
- **영구 장애**: 노드가 복구되지 않거나 데이터 유실 발생.
- **해결책**: 다른 정상 레플리카에서 데이터를 복제해 새로운 노드로 전달.
- **Anti-Entropy**(안티 엔트로피):
  - **Merkle Tree**(해시 트리) 비교로 서로 다른 부분만 동기화.
  - 네트워크 비용 최소화하면서 레플리카 상태를 점진적으로 일치시킴.


### 요약
- **감지**는 Gossip/Failure detector
- **일시적 장애**는 Sloppy Quorum + Hinted Handoff
- **영구 장애**는 Anti-Entropy + 재복제.


### 시스템 아키텍처 다이어그램
- **구성 요소**
  - **클라이언트**: 단순 API(get, put)와 통신
  - **중재자**: 요청 수신 -> 해시 링에서 파티션 결정 -> 레플리카 노드들에 요청 전파.
- 데이터는 여러 노드에 다중화된다.
- 모든 노드가 같은 책임을 지므로, SPOF는 존재하지 않는다.

### 쓰기 경로 (카산드라 사례)
- **커밋 로그(Commit Log)**
  - 쓰기 요청이 들어오면 먼저 커밋 로그 파일에 기록.
- **메모리 캐시(Memtable)**
  - 커밋 로그에 기록한 뒤, 데이터는 메모리 캐시(Memtable) 에 저장.
  - 빠른 접근이 가능하고, 디스크 flush 전까지 임시 저장소 역할.
- **SSTable(디스크)**
  - Memtable이 가득 차거나 임계치 도달 시 디스크에 SSTable 형태로 flush.
  - 디스크에 저장된 SSTable은 변경 불가능(Immutable).
  - 여러 SSTable은 주기적으로 컴팩션(compaction) 되어 병합/정리됨.

### 읽기 경로 (카산드라 사례)
- **Memtable 확인**
  - 먼저 메모리 캐시(Memtable)에 해당 키가 있는지 확인.
  - 있으면 바로 반환.
- **Bloom Filter 검사**
  - Memtable에 없다면, **블룸 필터(Bloom Filter)** 를 사용해 어떤 SSTable에 키가 있을 가능성이 있는지 확인.
  - 블룸 필터는 **존재하지 않는 키는 확실히 걸러내고**, 존재하는 키는 “있을 수도 있다”만 알려줌.
- **SSTable 조회**
  - 블룸 필터가 가리킨 SSTable을 열어 실제 데이터를 조회.
  - 필요 시 인덱스(Primary Index, Sparse Index) 사용해 검색 속도 향상.
- **데이터 반환**
  - 찾은 데이터를 클라이언트에게 반환.
  - 여러 SSTable에 걸쳐 있을 경우 가장 최신 타임스탬프 버전 선택.